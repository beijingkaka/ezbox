diff -urNd linux-2.6.32.32-adeos/arch/x86/kernel/entry_32.S linux-2.6.32.32-rtai/arch/x86/kernel/entry_32.S
--- linux-2.6.32.32-adeos/arch/x86/kernel/entry_32.S	2011-07-19 14:44:14.238271709 +0800
+++ linux-2.6.32.32-rtai/arch/x86/kernel/entry_32.S	2011-07-19 14:49:22.678272066 +0800
@@ -871,7 +871,7 @@
 	SAVE_ALL
 	IPIPE_TRACE_IRQ_ENTER
 	movl %esp, %eax
-	call __ipipe_handle_irq
+	call *ipipe_irq_handler
 	IPIPE_TRACE_IRQ_EXIT
 	testl %eax,%eax
 	jnz  ret_from_intr
@@ -886,7 +886,7 @@
 	SAVE_ALL;			\
 	IPIPE_TRACE_IRQ_ENTER;		\
  	movl %esp, %eax;		\
-	call __ipipe_handle_irq;	\
+	call *ipipe_irq_handler;	\
 	IPIPE_TRACE_IRQ_EXIT;		\
 	testl %eax,%eax;		\
 	jnz  ret_from_intr;		\
diff -urNd linux-2.6.32.32-adeos/arch/x86/kernel/entry_64.S linux-2.6.32.32-rtai/arch/x86/kernel/entry_64.S
--- linux-2.6.32.32-adeos/arch/x86/kernel/entry_64.S	2011-07-19 14:44:14.248271248 +0800
+++ linux-2.6.32.32-rtai/arch/x86/kernel/entry_64.S	2011-07-19 14:49:22.678272066 +0800
@@ -870,7 +870,7 @@
 #ifdef CONFIG_IPIPE
 	XCPT_FRAME
 	addq $-0x80,(%rsp)		/* Adjust vector to [-256,-1] range */
-	interrupt __ipipe_handle_irq
+	interrupt *ipipe_irq_handler
 	testl %eax, %eax
 	jnz ret_from_intr
 	decl PER_CPU_VAR(irq_count)
@@ -1032,7 +1032,7 @@
 	pushq $~(\num)
 	CFI_ADJUST_CFA_OFFSET 8
 #ifdef CONFIG_IPIPE
-	interrupt __ipipe_handle_irq
+	interrupt *ipipe_irq_handler
 	testl %eax, %eax
 	jnz ret_from_intr
 	decl PER_CPU_VAR(irq_count)
diff -urNd linux-2.6.32.32-adeos/arch/x86/kernel/ipipe.c linux-2.6.32.32-rtai/arch/x86/kernel/ipipe.c
--- linux-2.6.32.32-adeos/arch/x86/kernel/ipipe.c	2011-07-19 14:44:14.278271157 +0800
+++ linux-2.6.32.32-rtai/arch/x86/kernel/ipipe.c	2011-07-19 14:49:22.678272066 +0800
@@ -1050,6 +1050,30 @@
 	return 1;
 }
 
+void *ipipe_irq_handler = __ipipe_handle_irq;
+EXPORT_SYMBOL(ipipe_irq_handler);
+EXPORT_SYMBOL(io_apic_irqs);
+EXPORT_PER_CPU_SYMBOL(__ipipe_tick_regs);
+__attribute__((regparm(3))) void do_notify_resume(struct pt_regs *, void *, __u32);
+EXPORT_SYMBOL(do_notify_resume);
+extern void *sys_call_table;
+EXPORT_SYMBOL(sys_call_table);
+#ifdef CONFIG_X86_32
+extern void ret_from_intr(void);
+EXPORT_SYMBOL(ret_from_intr);
+extern spinlock_t i8259A_lock;
+extern struct desc_struct idt_table[];
+#else
+extern ipipe_spinlock_t i8259A_lock;
+extern gate_desc idt_table[];
+#endif
+EXPORT_PER_CPU_SYMBOL(vector_irq);
+EXPORT_SYMBOL(idt_table);
+EXPORT_SYMBOL(i8259A_lock);
+EXPORT_SYMBOL(__ipipe_sync_stage);
+EXPORT_SYMBOL(kill_proc_info);
+EXPORT_SYMBOL(find_task_by_pid_ns);
+
 EXPORT_SYMBOL(__ipipe_tick_irq);
 
 EXPORT_SYMBOL_GPL(irq_to_desc);
diff -urNd linux-2.6.32.32-adeos/kernel/exit.c linux-2.6.32.32-rtai/kernel/exit.c
--- linux-2.6.32.32-adeos/kernel/exit.c	2011-07-19 14:44:14.538271177 +0800
+++ linux-2.6.32.32-rtai/kernel/exit.c	2011-07-19 14:49:22.708274427 +0800
@@ -1786,3 +1786,37 @@
 }
 
 #endif
+
+void rt_daemonize(void)
+{
+	sigset_t blocked;
+
+	/*
+	 * We don't want to have TIF_FREEZE set if the system-wide hibernation
+	 * or suspend transition begins right now.
+	 */
+	current->flags |= (PF_NOFREEZE | PF_KTHREAD);
+
+	if (current->nsproxy != &init_nsproxy) {
+		get_nsproxy(&init_nsproxy);
+		switch_task_namespaces(current, &init_nsproxy);
+	}
+	set_special_pids(&init_struct_pid);
+	proc_clear_tty(current);
+
+	/* Block and flush all signals */
+	sigfillset(&blocked);
+	sigprocmask(SIG_BLOCK, &blocked, NULL);
+	flush_signals(current);
+
+	/* Become as one with the init task */
+
+	daemonize_fs_struct();
+	exit_files(current);
+	current->files = init_task.files;
+	atomic_inc(&current->files->count);
+
+	reparent_to_kthreadd();
+}
+
+EXPORT_SYMBOL(rt_daemonize);
diff -urNd linux-2.6.32.32-adeos/kernel/ipipe/core.c linux-2.6.32.32-rtai/kernel/ipipe/core.c
--- linux-2.6.32.32-adeos/kernel/ipipe/core.c	2011-07-19 14:44:14.558270640 +0800
+++ linux-2.6.32.32-rtai/kernel/ipipe/core.c	2011-07-19 14:49:22.718269150 +0800
@@ -1004,6 +1004,7 @@
 
 int __ipipe_dispatch_event (unsigned event, void *data)
 {
+extern void *ipipe_irq_handler; void *handler; if (ipipe_irq_handler != __ipipe_handle_irq && (handler = ipipe_root_domain->evhand[event])) { return ((int (*)(unsigned long, void *))handler)(event, data); } else {
 	struct ipipe_domain *start_domain, *this_domain, *next_domain;
 	struct ipipe_percpu_domain_data *np;
 	ipipe_event_handler_t evhand;
@@ -1075,7 +1076,7 @@
 	local_irq_restore_hw(flags);
 
 	return !propagate;
-}
+} }
 
 /*
  * __ipipe_dispatch_wired -- Wired interrupt dispatcher. Wired
@@ -1563,7 +1564,7 @@
 cpumask_t ipipe_set_irq_affinity (unsigned irq, cpumask_t cpumask)
 {
 #ifdef CONFIG_SMP
-	if (irq >= IPIPE_NR_XIRQS)
+	if (irq >= NR_IRQS) // if (irq >= IPIPE_NR_XIRQS)
 		/* Allow changing affinity of external IRQs only. */
 		return CPU_MASK_NONE;
 
diff -urNd linux-2.6.32.32-adeos/kernel/sched.c linux-2.6.32.32-rtai/kernel/sched.c
--- linux-2.6.32.32-adeos/kernel/sched.c	2011-07-19 14:44:14.698271924 +0800
+++ linux-2.6.32.32-rtai/kernel/sched.c	2011-07-19 14:49:22.728274297 +0800
@@ -3000,16 +3000,21 @@
  * context_switch - switch to the new MM and the new
  * thread's register state.
  */
-static inline int
+int
 context_switch(struct rq *rq, struct task_struct *prev,
 	       struct task_struct *next)
 {
 	struct mm_struct *mm, *oldmm;
 
-	prepare_task_switch(rq, prev, next);
-	trace_sched_switch(rq, prev, next);
 	mm = next->mm;
 	oldmm = prev->active_mm;
+
+if (!rq) {
+	switch_mm(oldmm, next->active_mm, next);
+	if (!mm) enter_lazy_tlb(oldmm, next);
+} else {
+	prepare_task_switch(rq, prev, next);
+	trace_sched_switch(rq, prev, next);
 	/*
 	 * For paravirt, this is coupled with an exit in switch_to to
 	 * combine the page table reload and the switch backend into
@@ -3037,19 +3042,23 @@
 #ifndef __ARCH_WANT_UNLOCKED_CTXSW
 	spin_release(&rq->lock.dep_map, 1, _THIS_IP_);
 #endif
-
+}
+#ifdef CONFIG_IPIPE
+	next->ptd[IPIPE_ROOT_NPTDKEYS - 1] = prev;
+#endif /* CONFIG_IPIPE */
 	/* Here we just switch the register state and the stack. */
 	switch_to(prev, next, prev);
 
 	barrier();
 
-#ifdef CONFIG_IPIPE_DELAYED_ATOMICSW
+if (unlikely(rq)) {
+#if 1 // def CONFIG_IPIPE_DELAYED_ATOMICSW
 	current->state &= ~TASK_ATOMICSWITCH;
 #else
 	prev->state &= ~TASK_ATOMICSWITCH;
 #endif
 	if (task_hijacked(prev))
-		return 1;
+		return 1; __ipipe_dispatch_event(IPIPE_FIRST_EVENT - 2, 0);
 
 	/*
 	 * this_rq must be evaluated again because prev may have moved
@@ -3057,10 +3066,12 @@
 	 * frame will be invalid.
 	 */
 	finish_task_switch(this_rq(), prev);
-
+}
 	return 0;
 }
 
+EXPORT_SYMBOL(context_switch);
+
 /*
  * nr_running, nr_uninterruptible and nr_context_switches:
  *
